\documentclass{article}

\newcommand{\size}{0.8\textwidth}

\newcommand{\image}[3]{
\begin{figure}
\begin{center}
\includegraphics[width=\size]{#1}
\caption{#2}
#3
\end{center}
\end{figure}
}

\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[official]{eurosym}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage[margin=3cm]{geometry}

% New Packages
\usepackage{hyperref}
% Referenz
\newcommand{\secref}[1]{\autoref{#1}. \textit{\nameref{#1}}}

\setlength{\parskip}{1em}
\setlength{\parindent}{0em}

\usepackage{floatrow}
% Table float box with bottom caption, box width adjusted to content
\newfloatcommand{capbtabbox}{table}[][\FBwidth]

\title{Exercise 2: Classfication}
\author{Rafaelt Sterzinger, Christian Stippel, Fatjon Zogaj}

\begin{document}
\maketitle

\section{Techniques}
Different performance techniques had been chosen, based on performance and on techniques previously used in the regression exercise. Thus, the K Nearest Neighbours Classifier and the Multi-Layer Perceptron Classifier were selected. A Decision Tree Classifier, which would have been the consecutive technique to the previously used Decision Tree Regressor, was used but early discarded since the Random Forest Classifier performed better. For the Kaggle Submissions, Support Vector Machines such as Linear Support Vector Classification and Support Vector Classification have been tried out as well, but they are not mentioned in the following report, since their performance were better than e.g. KNN, but still worse than MLP. In the following three subsections give an overview about the chosen classifiers.

\subsection{K Nearest Neighbours Classifier}
Classification with K Nearest Neighbours is an instance-based learning technique, which means that it does not construct a general internal model, but simple uses the different samples directly to classify. The class for a new sample is computed from a majority vote of the $k$ nearest neighbours of the new sample and therefore gets its most representative class assigned.

The more commonly used Classifier which uses a fixed $k$ instead of a fixed radius, was the chosen technique for K Nearest Neighbours. Since no general internal model is constructed, the choice of the value for a sample is highly data-dependent. Neighbours can have different weights based on distance or uniform weight, which influences the importance of a neighbours vote. 

Parameters altered:
\begin{itemize}
\item k
\item metric
\item weights
\end{itemize}

\subsection{Random Forest Classifier}
The Random Forest Classifier is an ensemble algorithm, which means that it consists of several build instances of estimators on random subsets of the original train data and their classification is then aggregated to give a final prediction. This helps to reduces the variance of a base estimator, which in the case of the Random Forest Classifier is a Decision Tree Classifier.

In many cases, bagging methods constitute a very simple way to improve with respect to a single model, without making it necessary to adapt the underlying base algorithm. As they provide a way to reduce overfitting, bagging methods work best with strong and complex models (e.g., fully developed decision trees), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees).

Bagging methods come in many flavours but mostly differ from each other by the way they draw random subsets of the training set:



%In scikit-learn, bagging methods are offered as a unified BaggingClassifier meta-estimator (resp. BaggingRegressor), taking as input a user-specified base estimator along with parameters specifying the strategy to draw random subsets. In particular, max_samples and max_features control the size of the subsets (in terms of samples and features), while bootstrap and bootstrap_features control whether samples and features are drawn with or without replacement. When using a subset of the available samples the generalization accuracy can be estimated with the out-of-bag samples by setting oob_score=True. As an example, the snippet below illustrates how to instantiate a bagging ensemble of KNeighborsClassifier base estimators, each built on random subsets of 50% of the samples and 50% of the features.

In random forests (see RandomForestClassifier and RandomForestRegressor classes), each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set.

%Furthermore, when splitting each node during the construction of a tree, the best split is found either from all input features or a random subset of size max_features. (See the parameter tuning guidelines for more details).

The purpose of these two sources of randomness is to decrease the variance of the forest estimator. Indeed, individual decision trees typically exhibit high variance and tend to overfit. The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant hence yielding an overall better model.

In contrast to the original publication [B2001], the scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class.

\subsection{Multi-Layer Perceptron Classifier}
% TODO: explain why k nearest neighbour, outline

\section{Performance Metrics}
% TODO: warum wir den gewählt haben für

\section{Breast-Cancer Dataset}
\input{breast/breast-cancer.tex}

\section{Amazon-Reviews Dataset}
\input{amazon/amazon-reviews.tex}

\section{Iris Dataset}
\input{iris/iris.tex}

\section{Online-Shoppers-Intention}
\input{onlineshop/onlineshop.tex}

\section{Conclusion}
%Overall Comparision of techniques

\end{document}
