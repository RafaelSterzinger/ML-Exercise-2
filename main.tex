\documentclass{article}

\newcommand{\size}{0.8\textwidth}

\newcommand{\image}[3]{
\begin{figure}
\begin{center}
\includegraphics[width=\size]{#1}
\caption{#2}
#3
\end{center}
\end{figure}
}

\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[official]{eurosym}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage[margin=3cm]{geometry}

% New Packages
\usepackage{hyperref}
% Referenz
\newcommand{\secref}[1]{\autoref{#1}. \textit{\nameref{#1}}}

\setlength{\parskip}{1em}
\setlength{\parindent}{0em}

\usepackage{floatrow}
% Table float box with bottom caption, box width adjusted to content
\newfloatcommand{capbtabbox}{table}[][\FBwidth]

\title{Exercise 2: Classfication}
\author{Rafaelt Sterzinger, Christian Stippel, Fatjon Zogaj}

\begin{document}
\maketitle

\section{Techniques}
Different performance techniques had been chosen, based on performance and on techniques previously used in the regression exercise. Thus, the K Nearest Neighbours Classifier and the Multi-Layer Perceptron Classifier were selected. A Decision Tree Classifier, which would have been the consecutive technique to the previously used Decision Tree Regressor, was used but early discarded since the Random Forest Classifier performed better. For the Kaggle Submissions, Support Vector Machines such as Linear Support Vector Classification and Support Vector Classification have been tried out as well, but they are not mentioned in the following report, since their performance were better than e.g. KNN, but still worse than MLP. In the following three subsections give an overview about the chosen classifiers.

\subsection{K Nearest Neighbours Classifier}
Classification with K Nearest Neighbours is an instance-based learning technique, which means that it does not construct a general internal model, but simple uses the different samples directly to classify. The class for a new sample is computed from a majority vote of the $k$ nearest neighbours of the new sample and therefore gets its most representative class assigned.

The more commonly used Classifier which uses a fixed $k$ instead of a fixed radius, was the chosen technique for K Nearest Neighbours. Since no general internal model is constructed, the choice of the value for a sample is highly data-dependent. Neighbours can have different weights based on distance or uniform weight, which influences the importance of a neighbours vote. 

Parameters altered:
\begin{itemize}
\item k
\item metric
\item weights
\end{itemize}

\subsection{Random Forest Classifier}
The Random Forest Classifier is an ensemble algorithm, which means that it consists of several build instances of estimators on random subsets of the original train data and their classification is then aggregated to give a final prediction. This helps to reduces the variance of a base estimator, which in the case of the Random Forest Classifier is a Decision Tree Classifier. Generating multiple estimators is also a simple way to avoid overfitting. Parameters such as max\_samples or max\_features control the size of the subsets, whereas bootstrapping decides if features/samples are drawn with or without replacement.

Parameters altered:
\begin{itemize}
\item n\_estimators
\item max\_features
\item min\_samples\_split
\item criterion
\end{itemize}

\subsection{Multi-Layer Perceptron Classifier}
A Multi-Layer Perceptron (MLP) is a supervised learning technique, which learns to approximate a function
by training on a dataset. It takes the attributes as an input and return a prediction for the target class and thus can learn a non-linear function for classification. Between the input and the output layer can also be multiple hidden layers, all consisting of neurons, which are connected with each other. Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation, followed by a non-linear activation function. Different activation function can be used, such as logistic, hyperbolic tan function or relu. A MLP has multiple advantages, e.g. it is sensitive to feature scaling which can be either good or bad, or parameter tuning which can take much time, especially depending on the amount of samples, features or layers.

Parameters altered:
\begin{itemize}
\item hidden\_layer\_sizes
\item activation
\item solver
\item learning\_rate
\item max\_iter
\end{itemize}

\section{Performance Metrics}
For the Performance Metrics accuracy, precision, recall and F1 were chosen. In the following section a short overview about each metric will be presented, however the question about the usefulness of a metric depending on a specific dataset is answered for each dataset on its own. In Figure \ref{fig:conf} the evaluation of a binary prediction is visualized, which is also known as the table of confusion. For multi label target values, it is also differed between micro and macro, which defines the way how multiple values are aggregate and when the average is calculated. In this report macro has been selected.

\image{performance.png}{Table of confusion}{\label{fig:conf}}
\begin{itemize}
\item $Accuracy = \frac{TP + TN}{TP + FP + FN + TN}$

\item $Precision = \frac{TP}{TP+FP} = P$

\item $Recall = \frac{TP}{TP+FN} = R$

\item $F1=2*\frac{P*R}{P+R}$
\end{itemize}

\section{Breast-Cancer Dataset}
\input{breast/breast-cancer.tex}

\section{Amazon-Reviews Dataset}
\input{amazon/amazon-reviews.tex}

\section{Iris Dataset}
\input{iris/iris.tex}

\section{Online-Shoppers-Intention}
\input{onlineshop/onlineshop.tex}

\section{Conclusion}
%Overall Comparision of techniques

\end{document}
