\documentclass{article}

\newcommand{\size}{0.6\textwidth}

\newcommand{\image}[3]{
\begin{figure}[H]
\begin{center}
\includegraphics[width=\size]{#1}
\caption{#2}
#3
\end{center}
\end{figure}
}

\newcommand{\multimage}[6]{
\begin{figure}[H]
\begin{center}
\begin{subfigure}{.45\textwidth}
\includegraphics[width=1.0\linewidth]{#1}
\caption{#2}
#3
\end{subfigure}
\begin{subfigure}{.45\textwidth}
\includegraphics[width=1.0\linewidth]{#4}
\caption{#5}
#6
\end{subfigure}
\caption{}
\end{center}
\end{figure}
}

\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[official]{eurosym}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage[margin=3cm]{geometry}
\usepackage{chngpage}
\usepackage{subcaption}


% New Packages
\usepackage{hyperref}
% Referenz
\newcommand{\secref}[1]{\autoref{#1}. \textit{\nameref{#1}}}

\setlength{\parskip}{1em}
\setlength{\parindent}{0em}

\usepackage{floatrow}
% Table float box with bottom caption, box width adjusted to content
\newfloatcommand{capbtabbox}{table}[][\FBwidth]

\title{Exercise 2: Classfication}
\author{Rafaelt Sterzinger, Christian Stippel, Fatjon Zogaj}

\begin{document}
\maketitle

\section{Techniques}
Different performance techniques had been chosen, based on performance and on techniques previously used in the regression exercise. Thus, the K Nearest Neighbours Classifier and the Multi-Layer Perceptron Classifier were selected. A Decision Tree Classifier, which would have been the consecutive technique to the previously used Decision Tree Regressor, was used but discarded early on, since the Random Forest Classifier performed better. For the Kaggle Submissions, Support Vector Machines such as Linear Support Vector Classification and Support Vector Classification have been tried out as well, but they are not mentioned in the following report, since their performance were worse than the best found estimators for each dataset. The following three subsections give an overview about the chosen classifiers.

\subsection{K Nearest Neighbours Classifier}
Classification with K Nearest Neighbours is an instance-based learning technique, which means that it does not construct a general internal model, but simple uses the different samples directly to classify. The class for a new sample is computed from a majority vote of the $k$ nearest neighbours of the new sample and therefore assigns the most representative class for it.

The more commonly used Classifier which uses a fixed $k$ instead of a fixed radius, was the chosen technique for K Nearest Neighbours. Since no general internal model is constructed, the choice of a label for a sample is highly data-dependent. Neighbours can have different weights based on distance or uniform weight, which influences the importance of a neighbours vote. 

Parameters altered:
\begin{itemize}
\item k
\item metric
\item weights
\end{itemize}

\subsection{Random Forest Classifier}
The Random Forest Classifier is an ensemble algorithm, which means that it consists of several build instances of estimators on random subsets of the original train data. Their choice of classification for a new sample is then aggregated to give a final prediction. This helps to reduce the variance of a base estimator, which in the case of the Random Forest Classifier is a Decision Tree Classifier. Generating multiple estimators is also a simple way to avoid overfitting. Parameters such as max\_samples or max\_features control the size of the subsets, whereas bootstrapping decides if features or samples are drawn with or without replacement.

Parameters altered:
\begin{itemize}
\item n\_estimators
\item max\_features
\item min\_samples\_split
\item criterion
\end{itemize}

\subsection{Multi-Layer Perceptron Classifier}
A Multi-Layer Perceptron (MLP) is a supervised learning technique, which learns to approximate a function
by training on a dataset. It takes the attributes as an input and returns a prediction for the target class and thus can learn a non-linear function for classification. Between the input and the output layer may also be multiple hidden layers, all consisting of neurons, which are connected with each other. Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation, followed by a non-linear activation function. Different activation functions can be used, such as logistic, hyperbolic tan function or relu. An MLP has multiple advantages, e.g. it is sensitive to feature scaling which can be either good or bad, or parameter tuning which can take much time. This is especially dependent on the amount of samples, features and layers.

Parameters altered:
\begin{itemize}
\item hidden\_layer\_sizes
\item activation
\item solver
\item learning\_rate
\item max\_iter
\end{itemize}

\section{Performance Metrics}
For the Performance Metrics, accuracy, precision, recall and F1 were chosen. In the following section a short overview about each metric will be presented, however the question about the usefulness of a metric depending on a specific dataset is answered for each dataset on its own in the respective sections. In Figure \ref{fig:conf} the evaluation of a binary prediction is visualized, which is also known as the table of confusion. For multi-label target values, it is also differed between micro and macro, which defines the way how multiple values are aggregated and when the average is calculated. In this report macro has been selected.

\image{performance.png}{Table of confusion}{\label{fig:conf}}
\begin{itemize}
\Large{
\item $Accuracy = \frac{TP + TN}{TP + FP + FN + TN}$

\item $Precision = \frac{TP}{TP+FP} = P$

\item $Recall = \frac{TP}{TP+FN} = R$

\item $F1=2*\frac{P*R}{P+R}$
}
\end{itemize}

\section{Amazon-Reviews Dataset}
\input{amazon/amazon-reviews.tex}

\section{Breast-Cancer Dataset}
\input{breast/breast-cancer.tex}

\section{Iris Dataset}
\input{iris/iris.tex}

\section{Online-Shoppers-Intention}
\input{onlineshop/onlineshop.tex}

% _________________________OVERALL CONCLUSION____________________________________________
\section{Overall Conclusion}
In this section we will summarize and analyse how useful the chosen classifiers are in regards to our specific datasets and how they compare to each other. \\
\newline
First of all it needs to be mentioned that the specific datasets pose different classification tasks as we have got 2 binary classifications (Online Shopping Predictions, Breast Cancer) and 2 Multiclass classifications (Iris, Amazon). Thus a direct comparison must be taken with a grain of salt, as having multiple classes adds another layer of complexity. \\
\newline
Looking at \secref{tab:summ} it is interesting to note that even the least sophisticated classifier, K Neighbors, was able to achieve 98 percent in every above mentioned performance metric for the Iris and Breast Cancer dataset. As seen in the previous plots, this can be explained by the good distinguishability of the specific features and allows the k Neighbors algorithm to look at the closest datapoints which are in the same baskets/classes. \\
Comparing K Neighbors  to the Random Forest Classifier, we see that more complexity does not have to lead to better results. For these 2 datasets the Random Forest Classifier actually performed marginally worse for every metric and on top of that took a lot longer relatively to fit. The Random Forest Classifier still is a good model as it performed very similar to K neighbors for these datasets. For the more complex datasets (Online Shopping Predictions, Amazon) it achieved much better scores than the K Neighbors Classifier and even had the highest accuracy and precision score for the Online Shopping Predictions dataset. \\
\newline
A slight trend/observation can be made out for the different classifiers: 
\begin{itemize}
    \item The K Neighbors Classifier performs well for simpler datasets, where the features are more distinguishable.
    \item Random Forest and MLP Classifier seem to perform well for simple and more complex datasets.
    \item The MLP Classifier performs better than the others when the underlying structure to be learned is more sophisticated.
\end{itemize} 
Concerning the question about which classifiers prove useful, one needs to define a threshold for when we say that a model is applicable for real world use. For the Iris and Breast Cancer dataset the choice of classifier does not make too much of a difference, especially considering the fact that we only have a few datapoints and a few more could skew the scores for one classifier or the other. \\
For the Online Shopping Predictions one could say that the Random Forest Classifier's results are good enough if one is only interested in the accuracy and the precision score. This could be the case if we say that the cost of false positives is high, for example if we'd send out expensive gifts for every potential buyer. Considering the fact that this model will most likely be used to do some form of marketing it is more plausible that we care more about the cost of false negatives, i.e. potentially losing customers. Thus for the Online Shopping Predictions dataset either a Random Forest or, more likely, the MLP Classifier could be applicable if one deems the achieved scores acceptable. \\
Regarding the Amazon dataset it is clear that the MLP Classifier performs best at it achieves scores which are 10 percent higher (for every metric) than the Random Forest Classifier and takes less than a fifth of the time to fit. Nevertheless, as even the best classifiers for Online Shopping Predictions and the Amazon dataset do not seem to have non substential error rates, one needs to take the model's real world applicability with a grain of salt. \\
\newline

\begin{table}
\begin{adjustwidth}{-3.0cm}{-3.0cm}  
\begin{center}
\begin{tabular}{|l|ccccr|ccccr|ccccr|}
\hline
                                                                      & \multicolumn{5}{c|}{K Neighbors} & \multicolumn{5}{c|}{Random Forest} & \multicolumn{5}{c|}{MLP} \\ %\hline
                                                                      & A    & P    & R    & F1    & sec   & A     & P    & R    & F1    & sec    & A   & P  & R  & F1  & sec  \\ \hline
Iris                                                                  & 0.98   & 0.98  & 0.98 & 0.98 & 0.0014 & 0.96   & 0.96    & 0.96 & 0.96 & 0.0695 & 0.98   & 0.98    & 0.98 & 0.98 & 0.7719   \\ \hline
\begin{tabular}[c]{@{}l@{}}Online Shopping\\ Predictions\end{tabular} & 0.85   & 0.55  & 0.51 & 0.52 & 0.0434 & 0.90   & 0.72    & 0.58   & 0.64   & 1.0573 & 0.89   & 0.66    & 0.62   & 0.64   & 5.3267   \\ \hline
Breast Cancer                                                         & 0.99   & 0.99    & 0.98 & 0.98 & 0.0016     & 0.98   & 0.98    & 0.97 & 0.97 & 0.0677      & 0.99   & 0.99    & 0.99 & 0.98 & 0.8999    \\ \hline
Amazon                                                                & 0.43 & 0.38 & 0.44 & 0.38 & 30.9870  & 0.59 & 0.54 & 0.59 & 0.54 & 481.1520 &  0.70 & 0.66 & 0.69 & 0.65 & 87.9900     \\ \hline
\end{tabular}
\caption{Performance Metrics and Runtime Summary} 
\label{tab:summ}
\end{center}
\end{adjustwidth}
\end{table}
\subsection{Parameter Sensitivity}
Analysing the sensitivity of the classifiers in regards to parameter changes, we have found that simply changing one parameter sometimes could completely turn the result into another direction. This was especially observable for the MLP Classifier where simply choosing adam as the optimizer instead of the regular stochastic gradient descent turned a recall score from 0.2 to 0.675 (see \secref{fig:mlp_sgd} and \secref{fig:mlp_adam}). On top of that the MLP was very sensitive in regards to the activation function with tangens hyperbolicus generally providing the best results and the logistic activation function generally performing the worst.\\
We have found that a higher amount of n\_estimators, i.e. trees, usually leads to a better performance up to a certain point. This can be explained by the fact that many trees help to create an average which does not overfit to certain scenarios.\\
In regards to K Neighbors we have seen that we tend to get the best results with Euclidean and Manhattan as the distance metric.  
Another thing that could be observed for the K Neighbors Classifier, was that preprocessing the data in general also lead to an increase of k, since with the scaled ranges  more datapoints could now be considered. \\
\newline
%TODO Preprocessing vs No-Preprocessing General Tabelle
\subsection{Preprocessing Effects}
Comparing the effects of Preprocessing, we see that the K Neighbors and MLP Classifier seemed to generally benefit the most from Scaling (excluding Iris since it only has a few features) while it did not have too much of of an impact on the Random Forest Classifier and even lead to a worse score for the Online Shopping Predictions dataset. For the Iris dataset, preprocessing lead to a decrease in performance for the K Neighbors Classifier and the MLP Classifier.
\subsection{Significance Testing}
Since one significance test against one baseline is mandatory, comparing the difference between the performance of the KNN as the baseline and the MLP would be the best application. Therefore the best estimator of KNN and the one of the MLP were selected for the Breast Cancer dataset, and their performance were evaluated again with cross validation with 10 splits each. To compare the significance, a related T-test was used, since both validation scores are calculated from the same sample. For the test, \textit{ttest\_rel}, from the \textit{scipy} packages was used, which is a two sided T-test. Thus, $H_0=0.9862$ and $H_1\neq0.9862$, which is the mean accuracy of KNN. The p-value returned by the method is 0.9727. For a threshold $\alpha$ of 5\%, the $H_0$ hypothesis of identical average scores can \textbf{not} be rejected. Therefore, both means are not significantly different.
\subsection{Runtime}
While the amount of samples played a big role in the runtimes of the different classifiers, the amount of selected features had the biggest impact. Obviously the number of samples affects how long/often the model needs to be trained and updated (MLP), but the number of features affect how long it takes to calculate the comparisons to the other datasets and how long one 'calculation cycle' is. Iris and Breast Cancer which contained a similar amount of samples/features and also had very similar scores also seemed to  have pretty much the exact same fitting times. Increasing the samplesize to around 12.000 (Online Shopping with around 40 times more samples than Breast Cancer's 300) led to a runtime increase by a factor of 30 for K Neighbors, 15 for Random Forest and 5 for MLP.\\ On the other hand increasing the feature amount by a factor of 100 (we selected over a thousand features for the Amazon dataset and only around 15 for Breast Cancer) we saw a runtime increase by a factor of 20.000 for K Neighbors, 7.000 for Random Forest and 100 for MLP (MLP: less features or else it would take too long). 
Comparing the high number of samples of the Online Shopping Predictions Dataset with the high number of features of the Amazon dataset runtime ratios of 700, 450 and 20 can be noted. Thus it seems that the runtime of K Neighbors seemed to be affected the most by an increase of samples/features, followed by Random Forest and then MLP. \\
\newline
Except for the Amazon dataset, where we achieved the best results with only a high amount of features, the MLP took around $c*10^1$ times longer to fit than the Random Forest Classifier and around $c* 10^2$ times longer than the K Neighbors Classifier.\\
Observing the Amazon Dataset we see that the ratio of $c*10^1$ seems to hold true if we compare Random Forest to K Neighbors Classifier. The runtime ratio MLP to Random Forest did not scale as it did for the other datasets and MLP was even faster than Random Forest, which could be explained by the parameter choice in the best case scenarios. \\
Another thing to keep in mind is that the parameters have got a huge influence on the runtimes. Increasing the amount of neighbors/amount of trees \& max depths/amount of iterations \& neurons for the respective classifiers also lead to an increase in runtimes.
\end{document}
