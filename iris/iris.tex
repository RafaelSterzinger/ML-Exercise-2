The Iris dataset was chosen, because of its low number of dimensions and its small number of samples, which fulfils the requirement to be contrary to the other selected set and is a good beginning example for classification. The aim of this dataset is to predict the species of a flower, depending on different measurement of sepal and petal. Furthermore, it is a very well known dataset, which allows comparison and therefore gives feedback on the accuracy of our results. In general, preprocessing is not mandatory, however since the dataset contains only rational data types, it would be interesting two compare the scaled data versus the non-scaled one. A selection of features is not necessary, since it only consists of four dimensions and already performs quite well. For the following analysis of the different classification techniques in regards with the Iris dataset, mainly \textit{GridSearchCV}, with cross validation from 3 to 10, depending on the complexity and therefore on the runtime, was used. 

\subsection{Characteristics}

\begin{itemize}
\item No missing values
\item 3 different target classes
\item Rational quantities (sepal length, sepal width, petal length, petal width)
\item 4 attributes
\item 150 samples
\end{itemize}

Comparing the correlation of the different attributes, \textit{petal width}, \textit{petal length} and \textit{sepal length} are all strong correlated. Only \textit{sepal width} is an exception, which is less and negatively correlated to the others (Figure \ref{fig:heat}). Thus, sepal width could be a good attribute to distinct between the different classes.

\image{iris/plots/heatmap.png}{Heatmap of the different attributes of the Iris dataset}{\label{fig:heat}}

\subsection{Characteristics of Target Value}
A comparison of the different classifiers in sepal width versus sepal length or petal width versus petal length shows, that the class Setosa is the most distinguishable (Figure \ref{petal}). Concerning the classes Versicolor and Virginica, they are rather unique when comparing them in petal width versus petal length, whereas in sepal width versus sepal length they are not separable. All three classes have the same occurrence, with 50 instances each.

\image{iris/plots/petal.png}{Comparison of Petal Length versus Petal Width}{\label{petal}}

\subsection{K Nearest Neighbours Classifier}
For K Nearest Neighbours a \textit{GridSearchCV} was carried out, where especially different \textit{k's} and metrics have been altered. Before that, a more general search has shown, that uniform weights perform better on the Iris dataset. Without preprocessing the best estimator is an Euclidean classifier, where \textit{k} equals 13. Comparing the different metrics, Chebyshev peaks earlier than euclidean, whereas Euclidean generally performed the best and Manhattan the worst (Figure \ref{knn}). 

\image{iris/plots/knn_np_comparision.png}{K Nearest Neighbours with different distance metrics}{\label{knn}}

With regards to the question about the effect of preprocessing, Figure \ref{prepro} shows clearly, that on the Iris dataset preprocessed data performs worse than non preprocessed. Figure \ref{prepro} compares each best estimator for preprocessed and non preprocessed. Both estimators use the Euclidean distance metric and differ only in \textit{k}, where for preprocessing \textit{k=30} and for non preprocessing \textit{k=13} performed the best.  For preprocessing solely a \textit{MinMaxScaler} has been used.

\image{iris/plots/knn_np_p_comparision.png}{Preprocessing versus No-Preprocessing}{\label{prepro}}

\subsection{Random Forest Classifier}
Concerning the Random Forest Classifier, a \textit{RandomizedSearchCV} was used to approximate the hyper parameters, such as the amount of estimators or the min samples split size, before a more specific search was done. This randomized search picked \textit{gini} as the better measurement of the quality of  a split and a min samples split of about 0.01, which was altered between 0.01 and 0.199. \\
The amount of different max features was only changed in the more specific \textit{GridSearchCV}, where previously found approximations of the parameters were used to keep the runtime shorter. Referring to Figure \ref{rf}, a real difference in performance seems to be only between one and more than one features. The amount of estimators do not make a big difference, but the search picked 80 as the best amount with max features of 2 for preprocessed and non preprocessed data. Both models archived the same performance, which is no surprise since scaling does not influence decision trees as well as a random forest.

\image{iris/plots/rf_np_comparision.png}{Comparison of the \textit{max\_features} attribute for different \textit{n\_estimators}}{\label{rf}}

\subsection{Multi-Layer Perceptron Classifier}
For the Multi-Layer Perceptron a strategy, similarly to the random forest one,  was chosen, where a general search was done by the \textit{RandomizedSearchCV}, followed by a more specific one with the \textit{GridSearchCV}.

For the randomized search, different solver, learning rates and starting alphas have been altered, to fixate some parameters, when conducting a more granular search, to keep the run time shorter. Concerning the solver, stochastic gradient descent came out on top instead of \textit{adam}, which is a different stochastic gradient-based optimizer. Altering the learning rates 0.01, 0.001, 0.0001, 0.001 performed the best, with constant learning rate.

Afterwards only different models and activation function were tried for scaled and non scaled data. The size and the amount of the different layers were randomly picked, where the sizes were chosen to be between the amount of input neurons and the amount of output neurons, which resulted in three different models:

\begin{itemize}
\item (3,4,3)
\item (4,4,4)
\item (4,3,4)
\end{itemize}

The second parameter which was varied, were the activation functions. There \textit{tanh, relu, logistic, itdenty} had been altered. When evaluating the different \textit{GridSeachCV} results with max iteration set 4000, the non preprocessed model performed slightly better, with an accuracy of 0.9866, which was the overall best performance of all different techniques. Interestingly, both searches for scaled and non scaled data, chose the tangent hyperbolic as the activation function, but concerning the model, preprocessed performed best with the (3,4,3) model and non preprocessed with the (4,3,4) one. In both cases the Logistic or Sigmoid activation function performed significantly worse, which could be due to the fact of vanishing gradients (Figure \ref{mlp}).

\image{iris/plots/mlp_np_comparision.png}{Comparison of different models and activation functions}{\label{mlp}}

\subsection{Conclusion}

Taking previously made findings about scaling in KNN into account, one can summarise that on the Iris dataset no preprocessed data performs slightly better than preprocessed.

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
                       & Preprocessing & No-Preprocessing \\ \hline
KNeighborsClassifier   & 0.9733        & 0.9800           \\ \hline
RandomForestClassifier & 0.9666        & 0.9666           \\ \hline
MLPClassifier          & 0.9666        & 0.9866           \\ \hline
\end{tabular}
\caption{Comparison of accuracy of different techniques with- and without preprocessing}
\end{center}
\end{table}

A comparison between holdout and cross validation gives mixed results and it is very questionable that one would get an accuracy of 100\% when testing. However, holdout was never completely different to cross validation and therefore can be useful for a quick approximation of the performance of a model, where cross validation would take much longer.

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
                       & Holdout & Cross Validation \\ \hline
KNeighborsClassifier   & 0.9333  & 0.9800           \\ \hline
RandomForestClassifier & 0.9666  & 0.9666           \\ \hline
MLPClassifier          & 1.0000  & 0.9866           \\ \hline
\end{tabular}
\caption{Comparison of accuracy of holdout versus cross-validation}
\end{center}
\end{table}

Concerning the performance metrics, accuracy is the most useful measurement for the Iris dateset set, because the target value is perfectly balanced, with 50 entries per class. Archiving a score of over 98\%, thus results in the conclusion, that the model is able to correctly classify all three categories. Therefore the MLPClassifer performed the best, but not only in accuracy, but in general. The runtime of each model is the average time it took to fit the test data for 10 folds. It is no surprise that fitting the test data to the most time for the Classifier

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
                       & Accuracy & Precision & Recall & F1     & Runtime (sec) \\ \hline
KNeighborsClassifier   & 0.9800   & 0.9833  & 0.9799 & 0.9797 & 0.0014        \\ \hline
RandomForestClassifier & 0.9600   & 0.9644    & 0.9600 & 0.9597 & 0.0695        \\ \hline
MLPClassifier          & 0.9800   & 0.9848    & 0.9800 & 0.9793 & 0.7719        \\ \hline
\end{tabular}
\caption{Comparison of different performance metrics and runtimes}
\end{center}
\end{table}
